{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97869d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.12/site-packages (25.2)\n",
      "Requirement already satisfied: ultralytics in ./venv/lib/python3.12/site-packages (8.3.174)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy>=1.23.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (3.10.5)\n",
      "Requirement already satisfied: pillow>=7.1.2 in ./venv/lib/python3.12/site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./venv/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./venv/lib/python3.12/site-packages (from ultralytics) (1.16.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.7.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.12/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in ./venv/lib/python3.12/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.3.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.0.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2025.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# to use virtual env\n",
    "# python3 -m venv venv\n",
    "# source venv/bin/activate\n",
    "\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install ultralytics opencv-python\n",
    "!pip3 install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04189ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset: https://www.kaggle.com/datasets/andrewmvd/car-plate-detection?resource=download\n",
    "# rename the folder: \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2283c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4155ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files split, grouped, and labels converted to YOLO TXT format\n"
     ]
    }
   ],
   "source": [
    "# process the annotations in YOLO text format\n",
    "# split the data into train 70%, val 20% and test 10%\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "image_dir = 'dataset/images'\n",
    "label_dir = 'dataset/annotations'\n",
    "\n",
    "out_base = 'dataset_split'\n",
    "\n",
    "def convert_xml_to_yolo(xml_path, txt_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    size = root.find('size')\n",
    "    width = float(size.find('width').text)\n",
    "    height = float(size.find('height').text)\n",
    "\n",
    "    lines = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_id = 0 \n",
    "\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = float(bndbox.find('xmin').text)\n",
    "        ymin = float(bndbox.find('ymin').text)\n",
    "        xmax = float(bndbox.find('xmax').text)\n",
    "        ymax = float(bndbox.find('ymax').text)\n",
    "\n",
    "        x_center = ((xmin + xmax) / 2) / width\n",
    "        y_center = ((ymin + ymax) / 2) / height\n",
    "        w = (xmax - xmin) / width\n",
    "        h = (ymax - ymin) / height\n",
    "\n",
    "        lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\")\n",
    "\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "base_names = [os.path.splitext(f)[0] for f in image_files]\n",
    "\n",
    "random.shuffle(base_names)\n",
    "total = len(base_names)\n",
    "train_end = int(0.7 * total)\n",
    "val_end = int(0.9 * total)\n",
    "\n",
    "splits = {\n",
    "    'train': base_names[:train_end],\n",
    "    'val': base_names[train_end:val_end],\n",
    "    'test': base_names[val_end:]\n",
    "}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(f'{out_base}/images/{split}', exist_ok=True)\n",
    "    os.makedirs(f'{out_base}/labels/{split}', exist_ok=True)\n",
    "\n",
    "for split, names in splits.items():\n",
    "    for name in names:\n",
    "        img_src = os.path.join(image_dir, f'{name}.png')\n",
    "        lbl_src = os.path.join(label_dir, f'{name}.xml')\n",
    "\n",
    "        img_dst = os.path.join(out_base, 'images', split, f'{name}.png')\n",
    "        txt_dst = os.path.join(out_base, 'labels', split, f'{name}.txt')\n",
    "\n",
    "        if os.path.exists(img_src):\n",
    "            shutil.copy(img_src, img_dst)\n",
    "\n",
    "        if os.path.exists(lbl_src):\n",
    "            convert_xml_to_yolo(lbl_src, txt_dst)\n",
    "\n",
    "print(\"Files split, grouped, and labels converted to YOLO TXT format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34953ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "080e50df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training yolov8n.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_split/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train8, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/neslyn/Desktop/dl_project/runs/detect/train8, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1015.3Â±200.0 MB/s, size: 575.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/train.cache... 303 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 794.7Â±305.8 MB/s, size: 559.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/neslyn/Desktop/dl_project/runs/detect/train8/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train8\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.462      3.249      1.354         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:57<00:00,  9.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92    0.00349      0.978      0.577       0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.416      2.132      1.169         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:47<00:00,  8.84s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.526      0.435      0.416      0.203\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/5         0G      1.433      1.816      1.179         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:39<00:00,  8.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.933      0.152      0.441      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/5         0G      1.349       1.63      1.167         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:43<00:00,  8.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.907      0.422      0.635      0.343\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        5/5         0G      1.337       1.62      1.172         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:51<00:00,  9.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92        0.9      0.586      0.764      0.422\n",
      "\n",
      "5 epochs completed in 0.256 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train8/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train8/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /Users/neslyn/Desktop/dl_project/runs/detect/train8/weights/best.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92        0.9      0.585      0.764      0.422\n",
      "Speed: 1.1ms preprocess, 170.1ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train8\u001b[0m\n",
      "\n",
      "Training yolov8s.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_split/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train9, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/neslyn/Desktop/dl_project/runs/detect/train9, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1237.8Â±1626.2 MB/s, size: 575.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/train.cache... 303 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2747.7Â±1552.6 MB/s, size: 559.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/neslyn/Desktop/dl_project/runs/detect/train9/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train9\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.523      3.617      1.331         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:55<00:00, 21.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.775      0.788      0.794      0.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.491      1.484      1.253         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [07:54<00:00, 25.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:39<00:00, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.796      0.641      0.681      0.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.475      1.157      1.266         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [07:17<00:00, 23.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:39<00:00, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.808      0.609      0.726      0.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.365      1.001      1.199         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:52<00:00, 21.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:36<00:00, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92       0.93      0.723      0.838      0.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G       1.34     0.9606      1.231         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:25<00:00, 20.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:34<00:00, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.644       0.67      0.659      0.319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.644 hours.\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train9/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train9/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /Users/neslyn/Desktop/dl_project/runs/detect/train9/weights/best.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:32<00:00, 10.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92       0.93      0.724      0.839      0.457\n",
      "Speed: 0.7ms preprocess, 367.6ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train9\u001b[0m\n",
      "\n",
      "Training yolov8m.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_split/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train10, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/neslyn/Desktop/dl_project/runs/detect/train10, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 169 layers, 25,856,899 parameters, 25,856,883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 869.2Â±364.6 MB/s, size: 575.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/train.cache... 303 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5575.8Â±1331.7 MB/s, size: 559.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/neslyn/Desktop/dl_project/runs/detect/train10/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train10\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.541      2.349      1.367         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [15:19<00:00, 48.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:15<00:00, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.335      0.533      0.329      0.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.556      1.453      1.324         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [15:11<00:00, 47.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:10<00:00, 23.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92    0.00975      0.674    0.00863    0.00455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.506      1.139      1.347         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [14:36<00:00, 46.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:13<00:00, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92        0.8      0.728      0.738      0.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.442      1.073      1.275         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [14:57<00:00, 47.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:08<00:00, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92    0.00192      0.326    0.00124   0.000547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.401      1.027      1.299         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [15:42<00:00, 49.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:15<00:00, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.693      0.467       0.47      0.236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 1.365 hours.\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train10/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train10/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating /Users/neslyn/Desktop/dl_project/runs/detect/train10/weights/best.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 92 layers, 25,840,339 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:11<00:00, 23.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92        0.8      0.728      0.739      0.384\n",
      "Speed: 0.9ms preprocess, 819.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train10\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>mAP50</th>\n",
       "      <th>Training Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yolov8n.pt</td>\n",
       "      <td>0.8997</td>\n",
       "      <td>0.5853</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7642</td>\n",
       "      <td>941.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yolov8s.pt</td>\n",
       "      <td>0.9301</td>\n",
       "      <td>0.7235</td>\n",
       "      <td>None</td>\n",
       "      <td>0.8386</td>\n",
       "      <td>2354.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yolov8m.pt</td>\n",
       "      <td>0.8001</td>\n",
       "      <td>0.7283</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7388</td>\n",
       "      <td>4992.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Precision  Recall F1 Score   mAP50  Training Time (s)\n",
       "0  yolov8n.pt     0.8997  0.5853     None  0.7642             941.91\n",
       "1  yolov8s.pt     0.9301  0.7235     None  0.8386            2354.00\n",
       "2  yolov8m.pt     0.8001  0.7283     None  0.7388            4992.39"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\"yolov8n.pt\", \"yolov8s.pt\", \"yolov8m.pt\"]\n",
    "data_yaml = \"dataset_split/data.yaml\"\n",
    "epochs = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model = YOLO(model_name)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    metrics = model.train(data=data_yaml, epochs=epochs)\n",
    "    end_time = time.time()\n",
    "    train_time = round(end_time - start_time, 2)\n",
    "    \n",
    "    map50 = metrics.results_dict.get(\"metrics/mAP50(B)\", None)\n",
    "    recall = metrics.results_dict.get(\"metrics/recall(B)\", None)\n",
    "    precision = metrics.results_dict.get(\"metrics/precision(B)\", None)\n",
    "    f1 = metrics.results_dict.get(\"metrics/f1(B)\", None)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision\": round(precision, 4) if precision else None,\n",
    "        \"Recall\": round(recall, 4) if recall else None,\n",
    "        \"F1 Score\": round(f1, 4) if f1 else None,\n",
    "        \"mAP50\": round(map50, 4) if map50 else None,\n",
    "        \"Training Time (s)\": train_time\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"yolo_model_comparison.csv\", index=False)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df750a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_split/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=7, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train11, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/neslyn/Desktop/dl_project/runs/detect/train11, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 705.7Â±266.5 MB/s, size: 575.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/train.cache... 303 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 558.9Â±95.3 MB/s, size: 559.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/neslyn/Desktop/dl_project/runs/detect/train11/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train11\u001b[0m\n",
      "Starting training for 7 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/7         0G      1.523      3.617      1.331         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:48<00:00, 21.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:36<00:00, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.775      0.788      0.794      0.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/7         0G      1.501      1.518      1.264         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:11<00:00, 19.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:33<00:00, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.729      0.652      0.663      0.303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/7         0G      1.503      1.184      1.298         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:12<00:00, 19.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:32<00:00, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.682      0.728      0.675      0.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/7         0G      1.458      1.053      1.262         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:47<00:00, 18.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:31<00:00, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.752      0.783      0.788      0.415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/7         0G      1.399      1.052      1.286         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:44<00:00, 18.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:31<00:00, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92       0.83      0.696      0.776      0.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        6/7         0G      1.399     0.9484       1.24         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:49<00:00, 18.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:32<00:00, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.749      0.707      0.776      0.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        7/7         0G       1.34     0.8621      1.184         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:08<00:00, 19.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:33<00:00, 11.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.845       0.88      0.899      0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7 epochs completed in 0.776 hours.\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train11/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train11/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /Users/neslyn/Desktop/dl_project/runs/detect/train11/weights/best.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:31<00:00, 10.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.844       0.88      0.899      0.463\n",
      "Speed: 0.7ms preprocess, 356.6ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train11\u001b[0m\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2412.8Â±1810.6 MB/s, size: 562.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.855       0.87      0.899      0.465\n",
      "Speed: 0.6ms preprocess, 340.2ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train112\u001b[0m\n",
      "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
      "\n",
      "ap_class_index: array([0])\n",
      "box: ultralytics.utils.metrics.Metric object\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x103ca0b30>\n",
      "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
      "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.98148,\n",
      "            0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,\n",
      "            0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,\n",
      "            0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,\n",
      "            0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,\n",
      "            0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,\n",
      "            0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,\n",
      "            0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,     0.98148,\n",
      "            0.98148,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,\n",
      "            0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,\n",
      "            0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,\n",
      "            0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.95312,     0.92537,     0.92537,     0.92537,     0.92537,\n",
      "            0.92537,     0.92537,     0.92537,     0.92537,     0.92537,     0.92537,     0.92537,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91429,\n",
      "            0.91429,     0.91429,     0.91429,     0.91429,     0.91429,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,\n",
      "            0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,\n",
      "            0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,\n",
      "            0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.91026,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,\n",
      "            0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.90244,     0.89286,\n",
      "            0.89286,     0.89286,     0.89286,     0.89286,     0.89286,     0.89286,     0.89286,     0.89286,     0.89286,     0.89286,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,\n",
      "            0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.88506,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,\n",
      "            0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,\n",
      "            0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.85417,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,\n",
      "            0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.79245,     0.70248,     0.70248,     0.70248,     0.70248,     0.70248,     0.70248,     0.70248,\n",
      "            0.70248,     0.70248,     0.70248,     0.55484,     0.55484,     0.55484,     0.55484,     0.55484,     0.55484,     0.55484,     0.55484,     0.55484,     0.55484,     0.55484,     0.53704,     0.53704,     0.53704,     0.53704,     0.53704,     0.53704,     0.53704,     0.53704,     0.53704,\n",
      "            0.53704,     0.53704,    0.081557,    0.081557,    0.081557,    0.081557,    0.081557,    0.081557,    0.081557,    0.081557,    0.081557,    0.081557,    0.081557,    0.014867,    0.014521,    0.014175,     0.01383,    0.013484,    0.013138,    0.012792,    0.012447,    0.012101,    0.011755,\n",
      "           0.011409,    0.011064,    0.010718,    0.010372,    0.010026,   0.0096807,   0.0093349,   0.0089892,   0.0086435,   0.0082977,    0.007952,   0.0076062,   0.0072605,   0.0069148,    0.006569,   0.0062233,   0.0058775,   0.0055318,   0.0051861,   0.0048403,   0.0044946,   0.0041489,   0.0038031,\n",
      "          0.0034574,   0.0031116,   0.0027659,   0.0024202,   0.0020744,   0.0017287,    0.001383,   0.0010372,  0.00069148,  0.00034574,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.02957,    0.029597,     0.09322,     0.17082,     0.22786,     0.26451,     0.29729,      0.3226,     0.34034,     0.36635,     0.37988,     0.40465,     0.41899,     0.43873,     0.44354,     0.45942,     0.47084,     0.48915,     0.49903,     0.50828,     0.51669,     0.52529,     0.53226,\n",
      "            0.53621,     0.54313,     0.55905,     0.56771,     0.57409,     0.57906,     0.59314,     0.60091,     0.60503,     0.60888,     0.61201,     0.62363,     0.62959,      0.6356,     0.63764,     0.63875,      0.6401,      0.6523,     0.65687,     0.65731,     0.65776,      0.6582,     0.65864,\n",
      "            0.65909,     0.66076,     0.66193,      0.6626,     0.66327,     0.66394,     0.66879,     0.67276,     0.67397,     0.67783,     0.67962,     0.68083,     0.68201,     0.68447,      0.6839,      0.6825,     0.68109,     0.68282,     0.68544,     0.68885,     0.69041,     0.69489,     0.69367,\n",
      "            0.69176,     0.70214,      0.7051,     0.71307,     0.71778,      0.7242,     0.72883,     0.73057,     0.73174,     0.73363,     0.73935,     0.74005,     0.74075,     0.74146,     0.74216,     0.74519,     0.74659,     0.74768,     0.74876,     0.74947,     0.75012,     0.75076,     0.75141,\n",
      "            0.75206,     0.75504,     0.76096,     0.76287,     0.76358,     0.76428,     0.76499,     0.76569,      0.7664,      0.7671,     0.76781,     0.76851,     0.76922,     0.77088,     0.77255,     0.78053,     0.78132,      0.7821,     0.78289,     0.78351,      0.7838,     0.78409,     0.78438,\n",
      "            0.78467,     0.78495,     0.78524,     0.78553,     0.78582,     0.78611,      0.7864,     0.78668,     0.78697,     0.79155,     0.79265,     0.79375,      0.7946,      0.7951,      0.7956,     0.79609,     0.79659,     0.79709,     0.79758,     0.79808,     0.79568,     0.79299,     0.79629,\n",
      "            0.79656,     0.79683,      0.7971,     0.79736,     0.79763,      0.7979,     0.79816,     0.79843,      0.7987,     0.79896,     0.79923,     0.79949,     0.79976,     0.80785,     0.80953,     0.81121,     0.81184,     0.81216,     0.81249,     0.81281,     0.81313,     0.81345,     0.81378,\n",
      "             0.8141,     0.81442,     0.81474,     0.81506,     0.81538,     0.81767,     0.81999,     0.82087,     0.82175,     0.82262,     0.82349,     0.82478,     0.82608,     0.82738,     0.82797,     0.82842,     0.82887,     0.82932,     0.82977,     0.83021,     0.83066,     0.83111,     0.83156,\n",
      "            0.83627,      0.8369,     0.83753,     0.83816,     0.83879,     0.83941,     0.84004,     0.84067,      0.8413,     0.84194,     0.84257,      0.8432,     0.84382,     0.84146,       0.837,     0.83758,     0.83848,     0.83937,     0.84027,     0.84116,     0.84201,     0.84286,     0.84371,\n",
      "            0.84456,     0.85418,     0.85437,     0.85457,     0.85477,     0.85497,     0.85516,     0.85536,     0.85556,     0.85576,     0.85595,     0.85615,     0.85635,     0.85654,     0.85674,     0.85694,     0.85713,     0.85733,     0.85753,     0.85772,     0.85792,     0.85812,     0.85831,\n",
      "            0.85851,     0.85992,     0.86395,     0.86778,     0.86804,      0.8683,     0.86856,     0.86881,     0.86907,     0.86933,     0.86958,     0.86984,      0.8701,     0.87035,     0.87061,     0.87087,     0.87112,     0.87138,     0.87163,     0.87189,     0.87215,     0.87218,     0.87149,\n",
      "             0.8708,     0.87012,     0.86943,     0.86874,     0.86805,     0.86736,     0.86666,     0.86574,     0.86458,     0.86342,     0.86225,     0.86108,     0.86026,     0.86043,     0.86059,     0.86076,     0.86093,     0.86109,     0.86126,     0.86143,     0.86159,     0.86176,     0.86193,\n",
      "            0.86209,     0.86226,     0.86242,     0.86259,     0.86276,     0.86292,     0.86309,     0.86325,     0.86342,     0.86359,     0.86375,     0.86392,     0.86408,     0.86425,     0.86441,     0.86458,     0.86475,     0.86544,     0.86753,     0.86956,     0.86917,     0.86878,      0.8684,\n",
      "            0.86801,     0.86762,     0.86724,     0.86685,     0.86646,     0.86607,     0.86569,      0.8653,     0.86491,     0.86452,     0.86413,     0.86374,     0.86336,     0.86306,     0.86276,     0.86246,     0.86216,     0.86186,     0.86156,     0.86126,     0.86096,     0.86066,     0.86036,\n",
      "            0.86006,     0.85976,     0.85946,     0.85916,     0.85886,     0.85856,     0.85825,     0.85795,     0.85765,     0.85735,     0.85702,     0.85661,     0.85621,     0.85581,     0.85541,       0.855,      0.8546,      0.8542,     0.85379,     0.85339,     0.85299,     0.85258,     0.85218,\n",
      "            0.85177,     0.85137,     0.85097,     0.85178,     0.85322,     0.85465,     0.85604,     0.85738,      0.8587,     0.86003,     0.86015,     0.85992,     0.85968,     0.85945,     0.85921,     0.85898,     0.85874,      0.8585,     0.85827,     0.85803,     0.85779,     0.85756,     0.85732,\n",
      "            0.85708,     0.85685,     0.85661,     0.85637,     0.85614,      0.8559,     0.85566,     0.85543,     0.85519,     0.85495,     0.85471,     0.85448,     0.85424,       0.854,     0.85353,     0.85295,     0.85238,      0.8518,     0.85123,     0.85065,     0.85008,      0.8495,     0.84892,\n",
      "            0.84834,     0.84777,     0.84752,     0.84766,      0.8478,     0.84794,     0.84808,     0.84822,     0.84836,      0.8485,     0.84864,     0.84878,     0.84892,     0.84906,      0.8492,     0.84934,     0.84947,     0.84961,     0.84975,     0.84989,     0.85003,     0.85017,     0.85031,\n",
      "            0.85045,     0.85059,     0.85072,     0.85086,       0.851,     0.85114,     0.85128,     0.85142,     0.85155,     0.85169,     0.85183,     0.85197,     0.85211,     0.85225,     0.85189,     0.85141,     0.85093,     0.85045,     0.84997,     0.84949,       0.849,     0.84852,     0.84804,\n",
      "            0.84756,     0.84708,     0.84659,     0.84611,     0.84635,     0.84977,     0.83306,     0.83182,     0.83454,     0.83481,     0.83415,     0.83348,     0.83281,     0.83214,     0.83147,      0.8308,     0.83013,     0.82946,     0.82879,     0.82826,     0.82791,     0.82757,     0.82722,\n",
      "            0.82688,     0.82653,     0.82619,     0.82584,      0.8255,     0.82515,     0.82481,     0.82446,     0.82412,     0.82377,     0.82343,     0.82308,     0.82273,     0.82239,     0.82204,     0.82169,     0.81421,     0.81353,     0.81286,     0.81218,      0.8115,     0.81082,     0.81014,\n",
      "            0.80946,     0.80878,      0.8081,     0.80742,     0.80642,      0.8053,     0.80417,     0.80304,     0.80191,     0.80078,     0.79923,     0.79672,      0.7942,      0.7926,     0.79239,     0.79218,     0.79198,     0.79177,     0.79156,     0.79135,     0.79114,     0.79093,     0.79073,\n",
      "            0.79052,     0.79031,      0.7901,     0.78989,     0.78968,     0.78948,     0.78927,     0.78906,     0.78885,     0.78864,     0.78843,     0.78822,     0.78801,      0.7878,     0.78759,     0.78738,     0.78718,     0.78697,     0.78676,     0.78655,     0.78634,     0.78613,     0.78592,\n",
      "            0.78571,      0.7855,     0.78529,     0.78539,     0.78552,     0.78564,     0.78576,     0.78589,     0.78601,     0.78613,     0.78626,     0.78638,      0.7865,     0.78663,     0.78675,     0.78687,       0.787,     0.78712,     0.78724,     0.78737,     0.78749,     0.78761,     0.78773,\n",
      "            0.78786,     0.78798,      0.7881,     0.78822,     0.78835,     0.78847,     0.78859,     0.78871,     0.78884,     0.78896,     0.78908,      0.7892,     0.78933,     0.78945,     0.78957,     0.78969,     0.78981,     0.78994,     0.79006,     0.78907,     0.78678,     0.78448,     0.78249,\n",
      "            0.78185,     0.78122,     0.78058,     0.77994,      0.7793,     0.77866,     0.77802,     0.77738,     0.77674,      0.7761,     0.77546,     0.77524,     0.77609,     0.77694,     0.77778,     0.77862,     0.77946,     0.77228,     0.77291,     0.77367,     0.77443,     0.77518,     0.77594,\n",
      "            0.77669,      0.7776,     0.77868,     0.77976,     0.78083,     0.78189,     0.78115,     0.78009,     0.77902,     0.77796,     0.77689,     0.77582,     0.77475,     0.77382,     0.77304,     0.77226,     0.77148,      0.7707,     0.76992,     0.76914,     0.76836,     0.76757,     0.76679,\n",
      "            0.76269,     0.75411,     0.74158,     0.74121,     0.74084,     0.74047,      0.7401,     0.73973,     0.73936,     0.73899,     0.73862,     0.73824,     0.73787,      0.7375,     0.73713,     0.73676,     0.73638,     0.73601,     0.73564,     0.73526,     0.73489,     0.73452,     0.73414,\n",
      "            0.73377,     0.73339,     0.73191,     0.73021,     0.72852,     0.72681,      0.7251,     0.72406,     0.72315,     0.72223,     0.72132,      0.7204,     0.71948,     0.71856,     0.71763,     0.71671,     0.71719,     0.71928,     0.72111,     0.72126,     0.72142,     0.72157,     0.72172,\n",
      "            0.72188,     0.72203,     0.72218,     0.72234,     0.72249,     0.72264,      0.7228,     0.72295,      0.7231,     0.72325,     0.72341,     0.72356,     0.72371,     0.72386,     0.72402,     0.72417,     0.72432,     0.72447,     0.72462,     0.72478,     0.72493,     0.72508,     0.72523,\n",
      "            0.72538,     0.72553,     0.72568,     0.72583,     0.72598,     0.72503,     0.72363,     0.72222,     0.72081,      0.7194,     0.71798,     0.71653,     0.71505,     0.71357,     0.71208,     0.71059,     0.70909,     0.69881,     0.69781,     0.69682,     0.69582,     0.69482,     0.69381,\n",
      "            0.69281,      0.6918,      0.6908,     0.68081,     0.67682,      0.6728,     0.66273,     0.66083,     0.65971,     0.65859,     0.65747,     0.65635,     0.65523,      0.6541,     0.65298,      0.6519,     0.65096,     0.65002,     0.64907,     0.64812,     0.64718,     0.64623,     0.64528,\n",
      "            0.64433,     0.64337,     0.64242,     0.63114,      0.6298,     0.62846,     0.62712,     0.62577,     0.62443,     0.62308,     0.62165,     0.62011,     0.61857,     0.61702,     0.61546,     0.61391,     0.61235,     0.61165,     0.61126,     0.61086,     0.61047,     0.61008,     0.60969,\n",
      "            0.60929,      0.6089,     0.60851,     0.60811,     0.60772,     0.60732,     0.60693,     0.60653,     0.60614,     0.60574,     0.60535,     0.60495,     0.60456,     0.60416,     0.60376,     0.60337,     0.60297,     0.60257,     0.60217,     0.60178,     0.59814,     0.59015,     0.58785,\n",
      "            0.58555,     0.58324,     0.58092,     0.58196,     0.58461,     0.56203,     0.55652,     0.55093,     0.54427,     0.53754,      0.5307,     0.52521,     0.52059,     0.51598,     0.51209,     0.50818,     0.50425,     0.48261,     0.46891,      0.4361,     0.42814,      0.4221,     0.41623,\n",
      "             0.4081,     0.39843,     0.38934,     0.37955,     0.37045,     0.36507,     0.35967,     0.35234,      0.3433,     0.32968,     0.32573,     0.32387,       0.322,     0.32013,     0.31826,     0.31638,      0.3145,     0.31262,     0.30764,     0.30085,     0.29547,     0.29297,     0.29047,\n",
      "            0.28797,     0.28545,     0.28293,      0.2804,     0.26283,     0.25511,     0.24743,     0.24261,     0.23775,     0.23287,     0.20795,     0.19737,     0.19117,     0.18556,     0.17991,      0.1719,     0.16284,     0.15892,     0.15736,      0.1558,     0.15423,     0.15266,     0.15109,\n",
      "            0.14951,     0.14793,     0.14635,     0.14477,     0.14318,     0.14159,     0.13813,     0.13442,      0.1307,     0.12696,     0.12321,     0.11836,      0.1132,     0.10802,     0.10227,    0.087028,    0.062572,    0.060212,    0.057847,    0.055475,    0.053098,    0.050715,    0.048326,\n",
      "           0.045931,    0.043531,    0.042306,    0.041889,    0.041472,    0.041054,    0.040637,    0.040219,    0.039801,    0.039383,    0.038965,    0.038546,    0.038128,    0.037709,     0.03729,    0.036871,    0.036451,    0.036032,    0.035612,    0.035192,    0.034772,    0.034352,    0.033931,\n",
      "           0.033511,     0.03309,    0.032669,    0.032248,    0.031827,    0.031405,    0.030983,    0.030562,     0.03014,    0.029717,    0.029295,    0.028872,     0.02845,    0.028027,    0.027603,     0.02718,    0.026757,    0.026333,    0.025909,    0.025485,    0.025061,    0.024636,    0.024212,\n",
      "           0.023787,    0.023362,    0.022937,    0.022512,    0.022086,    0.021661,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.015017,    0.015031,    0.048998,     0.09389,     0.12954,     0.15376,     0.17637,     0.19447,     0.20751,     0.22718,     0.23768,      0.2574,     0.26912,     0.28562,     0.28971,     0.30341,     0.31345,      0.3299,     0.33895,     0.34754,     0.35545,     0.36364,     0.37036,\n",
      "             0.3742,     0.38097,     0.39682,     0.40561,     0.41215,      0.4173,     0.43207,     0.44037,     0.44481,     0.44898,     0.45239,     0.46521,     0.47188,     0.47866,     0.48098,     0.48225,     0.48378,     0.49785,      0.5032,     0.50372,     0.50424,     0.50477,     0.50529,\n",
      "            0.50581,     0.50779,     0.50917,     0.50996,     0.51076,     0.51155,     0.51733,      0.5221,     0.52355,     0.52823,      0.5304,     0.53188,     0.53332,     0.53634,     0.53641,     0.53563,     0.53485,     0.53785,     0.54111,     0.54537,     0.54732,     0.55298,     0.55337,\n",
      "            0.55284,     0.56623,     0.57009,     0.58058,     0.58685,     0.59547,     0.60176,     0.60414,     0.60575,     0.60834,     0.61624,     0.61722,      0.6182,     0.61918,     0.62016,      0.6244,     0.62637,      0.6279,     0.62944,     0.63043,     0.63135,     0.63227,     0.63319,\n",
      "            0.63411,     0.63836,     0.64686,     0.64964,     0.65066,     0.65169,     0.65271,     0.65374,     0.65477,      0.6558,     0.65683,     0.65786,     0.65889,     0.66133,     0.66381,     0.67566,     0.67685,     0.67804,     0.67922,     0.68015,     0.68059,     0.68102,     0.68146,\n",
      "            0.68189,     0.68233,     0.68277,      0.6832,     0.68364,     0.68408,     0.68451,     0.68495,     0.68539,     0.69236,     0.69405,     0.69573,     0.69704,     0.69781,     0.69857,     0.69934,     0.70011,     0.70088,     0.70165,     0.70241,     0.70141,     0.70024,     0.70602,\n",
      "            0.70644,     0.70686,     0.70728,      0.7077,     0.70812,     0.70854,     0.70896,     0.70938,      0.7098,     0.71022,     0.71064,     0.71106,     0.71148,     0.72439,      0.7271,     0.72981,     0.73084,     0.73136,     0.73188,     0.73241,     0.73293,     0.73345,     0.73398,\n",
      "             0.7345,     0.73502,     0.73555,     0.73607,     0.73659,     0.74033,     0.74415,      0.7456,     0.74705,     0.74849,     0.74994,     0.75208,     0.75425,     0.75642,     0.75739,     0.75815,      0.7589,     0.75966,     0.76041,     0.76116,     0.76192,     0.76267,     0.76343,\n",
      "            0.77141,     0.77249,     0.77356,     0.77463,      0.7757,     0.77677,     0.77784,     0.77893,     0.78002,      0.7811,     0.78219,     0.78328,     0.78436,     0.79007,     0.78855,     0.78996,     0.79156,     0.79316,     0.79476,     0.79635,     0.79788,     0.79941,     0.80094,\n",
      "            0.80247,     0.82002,     0.82038,     0.82075,     0.82111,     0.82148,     0.82184,     0.82221,     0.82257,     0.82294,      0.8233,     0.82366,     0.82403,     0.82439,     0.82476,     0.82512,     0.82549,     0.82585,     0.82622,     0.82658,     0.82695,     0.82731,     0.82767,\n",
      "            0.82804,     0.83068,     0.83822,     0.84547,     0.84596,     0.84645,     0.84694,     0.84743,     0.84792,     0.84841,      0.8489,     0.84939,     0.84988,     0.85037,     0.85086,     0.85135,     0.85184,     0.85232,     0.85281,      0.8533,     0.85379,     0.85413,     0.85395,\n",
      "            0.85377,      0.8536,     0.85342,     0.85325,     0.85307,      0.8529,     0.85272,     0.85249,     0.85219,     0.85189,     0.85159,     0.85129,     0.85115,     0.85147,      0.8518,     0.85213,     0.85246,     0.85278,     0.85311,     0.85344,     0.85376,     0.85409,     0.85442,\n",
      "            0.85475,     0.85507,      0.8554,     0.85573,     0.85605,     0.85638,     0.85671,     0.85703,     0.85736,     0.85769,     0.85802,     0.85834,     0.85867,       0.859,     0.85932,     0.85965,     0.85998,     0.86136,     0.86551,     0.86956,     0.86947,     0.86938,     0.86929,\n",
      "             0.8692,     0.86911,     0.86902,     0.86893,     0.86884,     0.86875,     0.86866,     0.86857,     0.86848,     0.86839,      0.8683,     0.86821,     0.86813,     0.86806,     0.86798,     0.86791,     0.86784,     0.86777,      0.8677,     0.86763,     0.86756,     0.86749,     0.86742,\n",
      "            0.86735,     0.86728,     0.86721,     0.86714,     0.86707,       0.867,     0.86693,     0.86686,     0.86679,     0.86671,     0.86664,     0.86654,     0.86644,     0.86635,     0.86625,     0.86616,     0.86606,     0.86597,     0.86587,     0.86577,     0.86568,     0.86558,     0.86549,\n",
      "            0.86539,      0.8653,      0.8652,     0.86715,     0.87013,     0.87311,     0.87602,     0.87882,     0.88161,     0.88441,     0.88502,     0.88497,     0.88492,     0.88487,     0.88482,     0.88477,     0.88472,     0.88467,     0.88462,     0.88458,     0.88453,     0.88448,     0.88443,\n",
      "            0.88438,     0.88433,     0.88428,     0.88423,     0.88418,     0.88413,     0.88408,     0.88403,     0.88398,     0.88393,     0.88388,     0.88383,     0.88378,     0.88374,     0.88363,     0.88351,     0.88339,     0.88327,     0.88315,     0.88303,      0.8829,     0.88278,     0.88266,\n",
      "            0.88254,     0.88242,     0.88249,      0.8828,      0.8831,      0.8834,     0.88371,     0.88401,     0.88431,     0.88462,     0.88492,     0.88522,     0.88552,     0.88583,     0.88613,     0.88643,     0.88674,     0.88704,     0.88734,     0.88765,     0.88795,     0.88825,     0.88855,\n",
      "            0.88886,     0.88916,     0.88946,     0.88977,     0.89007,     0.89037,     0.89068,     0.89098,     0.89128,     0.89158,     0.89189,     0.89219,     0.89249,      0.8928,     0.89278,     0.89269,     0.89259,      0.8925,      0.8924,     0.89231,     0.89221,     0.89212,     0.89202,\n",
      "            0.89193,     0.89183,     0.89174,     0.89164,     0.89298,     0.90064,     0.89923,     0.90204,     0.90847,     0.91017,     0.91006,     0.90995,     0.90984,     0.90972,     0.90961,      0.9095,     0.90938,     0.90927,     0.90916,     0.90907,     0.90901,     0.90895,     0.90889,\n",
      "            0.90883,     0.90877,     0.90871,     0.90865,     0.90859,     0.90853,     0.90847,     0.90841,     0.90835,      0.9083,     0.90824,     0.90818,     0.90812,     0.90806,       0.908,     0.90794,     0.90664,     0.90652,      0.9064,     0.90628,     0.90616,     0.90604,     0.90592,\n",
      "             0.9058,     0.90568,     0.90556,     0.90544,     0.90526,     0.90506,     0.90486,     0.90465,     0.90445,     0.90425,     0.90397,     0.90351,     0.90305,     0.90276,     0.90272,     0.90269,     0.90265,     0.90261,     0.90257,     0.90253,     0.90249,     0.90245,     0.90241,\n",
      "            0.90238,     0.90234,      0.9023,     0.90226,     0.90222,     0.90218,     0.90214,     0.90211,     0.90207,     0.90203,     0.90199,     0.90195,     0.90191,     0.90187,     0.90184,      0.9018,     0.90176,     0.90172,     0.90168,     0.90164,      0.9016,     0.90157,     0.90153,\n",
      "            0.90149,     0.90145,     0.90141,     0.90172,     0.90204,     0.90237,     0.90269,     0.90302,     0.90335,     0.90367,       0.904,     0.90432,     0.90465,     0.90498,      0.9053,     0.90563,     0.90596,     0.90628,     0.90661,     0.90693,     0.90726,     0.90759,     0.90791,\n",
      "            0.90824,     0.90857,     0.90889,     0.90922,     0.90954,     0.90987,      0.9102,     0.91052,     0.91085,     0.91117,      0.9115,     0.91183,     0.91215,     0.91248,     0.91281,     0.91313,     0.91346,     0.91378,     0.91411,     0.91411,     0.91373,     0.91335,     0.91302,\n",
      "            0.91292,     0.91281,      0.9127,     0.91259,     0.91249,     0.91238,     0.91227,     0.91216,     0.91206,     0.91195,     0.91184,     0.91244,     0.91479,     0.91715,      0.9195,     0.92185,      0.9242,     0.92426,     0.92641,      0.9286,     0.93079,     0.93297,     0.93516,\n",
      "            0.93735,     0.94001,     0.94317,     0.94633,      0.9495,     0.95266,     0.95304,     0.95294,     0.95284,     0.95274,     0.95264,     0.95253,     0.95243,     0.95234,     0.95227,     0.95219,     0.95212,     0.95204,     0.95197,     0.95189,     0.95182,     0.95174,     0.95167,\n",
      "            0.95126,     0.95041,     0.94914,      0.9491,     0.94906,     0.94902,     0.94898,     0.94894,      0.9489,     0.94887,     0.94883,     0.94879,     0.94875,     0.94871,     0.94867,     0.94863,     0.94859,     0.94855,     0.94852,     0.94848,     0.94844,      0.9484,     0.94836,\n",
      "            0.94832,     0.94828,     0.94812,     0.94794,     0.94776,     0.94758,      0.9474,     0.94728,     0.94718,     0.94708,     0.94698,     0.94688,     0.94678,     0.94668,     0.94658,     0.94648,     0.94985,      0.9572,      0.9637,     0.96426,     0.96481,     0.96536,     0.96591,\n",
      "            0.96646,     0.96701,     0.96756,     0.96811,     0.96866,     0.96921,     0.96976,     0.97031,     0.97086,     0.97141,     0.97196,     0.97251,     0.97306,     0.97361,     0.97417,     0.97472,     0.97527,     0.97582,     0.97637,     0.97692,     0.97747,     0.97802,     0.97857,\n",
      "            0.97912,     0.97967,     0.98022,     0.98077,     0.98132,     0.98144,     0.98139,     0.98133,     0.98127,     0.98122,     0.98116,      0.9811,     0.98104,     0.98098,     0.98092,     0.98086,      0.9808,     0.98037,     0.98033,     0.98029,     0.98024,      0.9802,     0.98016,\n",
      "            0.98011,     0.98007,     0.98003,     0.97959,     0.97941,     0.97923,     0.97876,     0.97867,     0.97862,     0.97857,     0.97851,     0.97846,     0.97841,     0.97835,      0.9783,     0.97825,      0.9782,     0.97815,     0.97811,     0.97806,     0.97801,     0.97797,     0.97792,\n",
      "            0.97787,     0.97783,     0.97778,     0.97721,     0.97714,     0.97707,       0.977,     0.97693,     0.97686,     0.97679,     0.97671,     0.97663,     0.97655,     0.97646,     0.97638,      0.9763,     0.97621,     0.97617,     0.97615,     0.97613,     0.97611,     0.97609,     0.97606,\n",
      "            0.97604,     0.97602,       0.976,     0.97598,     0.97595,     0.97593,     0.97591,     0.97589,     0.97587,     0.97584,     0.97582,      0.9758,     0.97578,     0.97576,     0.97573,     0.97571,     0.97569,     0.97567,     0.97565,     0.97562,     0.97542,     0.97495,     0.97482,\n",
      "            0.97468,     0.97454,      0.9744,     0.98464,     0.99998,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.95652,     0.95652,     0.95652,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,\n",
      "            0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,\n",
      "            0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.94327,     0.94033,     0.93739,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.92928,\n",
      "            0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,\n",
      "            0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,\n",
      "            0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.91922,     0.91408,     0.91304,\n",
      "            0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,\n",
      "            0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,\n",
      "            0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,     0.91304,         0.9,      0.8918,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,\n",
      "             0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,\n",
      "             0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,     0.89101,     0.88977,\n",
      "            0.88853,     0.88728,     0.88604,      0.8848,     0.88356,     0.88231,     0.88107,     0.87942,     0.87734,     0.87527,     0.87319,     0.87111,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,\n",
      "            0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86955,     0.86887,     0.86819,      0.8675,\n",
      "            0.86682,     0.86614,     0.86546,     0.86478,     0.86409,     0.86341,     0.86273,     0.86205,     0.86137,     0.86068,        0.86,     0.85932,     0.85865,     0.85813,      0.8576,     0.85708,     0.85656,     0.85603,     0.85551,     0.85499,     0.85446,     0.85394,     0.85342,\n",
      "            0.85289,     0.85237,     0.85185,     0.85132,      0.8508,     0.85028,     0.84975,     0.84923,     0.84871,     0.84818,     0.84761,     0.84691,     0.84622,     0.84552,     0.84483,     0.84413,     0.84344,     0.84275,     0.84205,     0.84136,     0.84066,     0.83997,     0.83927,\n",
      "            0.83858,     0.83788,     0.83719,     0.83696,     0.83696,     0.83696,     0.83696,     0.83696,     0.83696,     0.83696,     0.83665,     0.83625,     0.83584,     0.83544,     0.83504,     0.83464,     0.83424,     0.83384,     0.83343,     0.83303,     0.83263,     0.83223,     0.83183,\n",
      "            0.83143,     0.83102,     0.83062,     0.83022,     0.82982,     0.82942,     0.82902,     0.82861,     0.82821,     0.82781,     0.82741,     0.82701,     0.82661,      0.8262,      0.8254,     0.82443,     0.82347,      0.8225,     0.82153,     0.82057,      0.8196,     0.81863,     0.81767,\n",
      "             0.8167,     0.81573,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,\n",
      "            0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81522,     0.81458,     0.81378,     0.81298,     0.81218,     0.81138,     0.81059,     0.80979,     0.80899,     0.80819,\n",
      "            0.80739,      0.8066,      0.8058,       0.805,     0.80435,     0.80435,     0.77596,     0.77174,     0.77174,     0.77098,     0.76992,     0.76887,     0.76781,     0.76675,      0.7657,     0.76464,     0.76359,     0.76253,     0.76147,     0.76064,      0.7601,     0.75956,     0.75902,\n",
      "            0.75849,     0.75795,     0.75741,     0.75687,     0.75633,     0.75579,     0.75526,     0.75472,     0.75418,     0.75364,      0.7531,     0.75256,     0.75202,     0.75149,     0.75095,     0.75041,     0.73888,     0.73785,     0.73682,     0.73578,     0.73475,     0.73372,     0.73268,\n",
      "            0.73165,     0.73062,     0.72958,     0.72855,     0.72704,     0.72534,     0.72365,     0.72195,     0.72026,     0.71856,     0.71624,      0.7125,     0.70876,      0.7064,     0.70609,     0.70579,     0.70548,     0.70517,     0.70487,     0.70456,     0.70425,     0.70395,     0.70364,\n",
      "            0.70333,     0.70303,     0.70272,     0.70241,     0.70211,      0.7018,     0.70149,     0.70119,     0.70088,     0.70057,     0.70027,     0.69996,     0.69966,     0.69935,     0.69904,     0.69874,     0.69843,     0.69812,     0.69782,     0.69751,      0.6972,      0.6969,     0.69659,\n",
      "            0.69628,     0.69598,     0.69567,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,\n",
      "            0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69565,     0.69412,      0.6908,     0.68748,     0.68461,\n",
      "             0.6837,     0.68279,     0.68187,     0.68096,     0.68005,     0.67913,     0.67822,      0.6773,     0.67639,     0.67548,     0.67456,     0.67391,     0.67391,     0.67391,     0.67391,     0.67391,     0.67391,     0.66323,     0.66304,     0.66304,     0.66304,     0.66304,     0.66304,\n",
      "            0.66304,     0.66304,     0.66304,     0.66304,     0.66304,     0.66304,     0.66179,     0.66031,     0.65884,     0.65736,     0.65589,     0.65441,     0.65294,     0.65166,     0.65059,     0.64952,     0.64846,     0.64739,     0.64632,     0.64526,     0.64419,     0.64312,     0.64206,\n",
      "            0.63651,     0.62501,     0.60852,     0.60803,     0.60755,     0.60707,     0.60659,      0.6061,     0.60562,     0.60514,     0.60466,     0.60417,     0.60369,     0.60321,     0.60273,     0.60224,     0.60176,     0.60128,      0.6008,     0.60031,     0.59983,     0.59935,     0.59887,\n",
      "            0.59838,      0.5979,       0.596,     0.59382,     0.59165,     0.58948,      0.5873,     0.58598,     0.58482,     0.58366,      0.5825,     0.58134,     0.58018,     0.57903,     0.57787,     0.57671,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,\n",
      "            0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57609,\n",
      "            0.57609,     0.57609,     0.57609,     0.57609,     0.57609,     0.57485,      0.5731,     0.57136,     0.56961,     0.56787,     0.56613,     0.56435,     0.56253,     0.56072,      0.5589,     0.55709,     0.55527,     0.54289,      0.5417,     0.54052,     0.53933,     0.53814,     0.53695,\n",
      "            0.53576,     0.53457,     0.53338,     0.52169,     0.51707,     0.51245,     0.50097,     0.49882,     0.49757,     0.49631,     0.49505,      0.4938,     0.49254,     0.49128,     0.49002,     0.48883,     0.48778,     0.48673,     0.48569,     0.48464,     0.48359,     0.48254,      0.4815,\n",
      "            0.48045,      0.4794,     0.47835,     0.46608,     0.46464,      0.4632,     0.46175,     0.46031,     0.45887,     0.45743,     0.45592,     0.45428,     0.45264,       0.451,     0.44936,     0.44772,     0.44608,     0.44535,     0.44494,     0.44452,     0.44411,      0.4437,     0.44329,\n",
      "            0.44288,     0.44247,     0.44206,     0.44165,     0.44123,     0.44082,     0.44041,        0.44,     0.43959,     0.43918,     0.43877,     0.43836,     0.43794,     0.43753,     0.43712,     0.43671,      0.4363,     0.43589,     0.43548,     0.43507,     0.43131,     0.42314,     0.42081,\n",
      "            0.41847,     0.41614,     0.41381,     0.41304,     0.41304,     0.39085,     0.38554,      0.3802,     0.37389,     0.36756,      0.3612,     0.35613,     0.35189,     0.34769,     0.34417,     0.34064,     0.33712,     0.31805,     0.30626,     0.27886,     0.27238,     0.26751,     0.26281,\n",
      "            0.25636,     0.24878,     0.24173,     0.23423,     0.22733,      0.2233,     0.21926,     0.21384,     0.20722,     0.19738,     0.19455,     0.19322,      0.1919,     0.19057,     0.18925,     0.18792,     0.18659,     0.18527,     0.18178,     0.17706,     0.17334,     0.17163,     0.16991,\n",
      "             0.1682,     0.16649,     0.16477,     0.16306,      0.1513,      0.1462,     0.14118,     0.13805,     0.13491,     0.13178,     0.11604,     0.10949,     0.10569,     0.10227,    0.098845,    0.094035,    0.088635,    0.086322,    0.085401,     0.08448,    0.083559,    0.082637,    0.081716,\n",
      "           0.080795,    0.079874,    0.078953,    0.078032,    0.077111,     0.07619,     0.07419,    0.072055,     0.06992,    0.067785,     0.06565,    0.062902,    0.059998,    0.057094,    0.053892,    0.045494,    0.032297,    0.031041,    0.029785,    0.028529,    0.027273,    0.026017,    0.024761,\n",
      "           0.023506,     0.02225,     0.02161,    0.021392,    0.021175,    0.020957,     0.02074,    0.020522,    0.020305,    0.020087,    0.019869,    0.019652,    0.019434,    0.019217,    0.018999,    0.018782,    0.018564,    0.018346,    0.018129,    0.017911,    0.017694,    0.017476,    0.017259,\n",
      "           0.017041,    0.016823,    0.016606,    0.016388,    0.016171,    0.015953,    0.015735,    0.015518,      0.0153,    0.015083,    0.014865,    0.014648,     0.01443,    0.014212,    0.013995,    0.013777,     0.01356,    0.013342,    0.013125,    0.012907,    0.012689,    0.012472,    0.012254,\n",
      "           0.012037,    0.011819,    0.011602,    0.011384,    0.011166,    0.010949,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
      "fitness: np.float64(0.5087242690843861)\n",
      "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
      "maps: array([    0.46541])\n",
      "names: {0: 'licence'}\n",
      "nt_per_class: array([92])\n",
      "nt_per_image: array([86])\n",
      "results_dict: {'metrics/precision(B)': np.float64(0.8553995039759215), 'metrics/recall(B)': np.float64(0.8695652173913043), 'metrics/mAP50(B)': np.float64(0.8985466635168649), 'metrics/mAP50-95(B)': np.float64(0.4654106697029995), 'fitness': np.float64(0.5087242690843861)}\n",
      "save_dir: PosixPath('/Users/neslyn/Desktop/dl_project/runs/detect/train112')\n",
      "speed: {'preprocess': 0.5583183140748419, 'inference': 340.19474079087917, 'loss': 7.559296940345057e-05, 'postprocess': 0.6425935119528562}\n",
      "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
      "task: 'detect'\n"
     ]
    }
   ],
   "source": [
    "# train and validate model\n",
    "# from the table above I could see yolo v8 small performs better\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "model.train(data=\"dataset_split/data.yaml\", epochs=7)\n",
    "metrics = model.val(data=\"dataset_split/data.yaml\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bc72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 16:49:46.825 Python[62885:2526145] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "1/1: 0... Failed to read images from 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl_project/venv/lib/python3.12/site-packages/ultralytics/engine/model.py:555\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl_project/venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:227\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl_project/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:36\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl_project/venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:300\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.setup_model(model)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_txt:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl_project/venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:259\u001b[39m, in \u001b[36mBasePredictor.setup_source\u001b[39m\u001b[34m(self, source)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[33;03mSet up source and inference mode.\u001b[39;00m\n\u001b[32m    253\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m \u001b[33;03m        Source for inference.\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;28mself\u001b[39m.imgsz = check_imgsz(\u001b[38;5;28mself\u001b[39m.args.imgsz, stride=\u001b[38;5;28mself\u001b[39m.model.stride, min_dim=\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m \u001b[38;5;28mself\u001b[39m.dataset = \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28mself\u001b[39m.source_type = \u001b[38;5;28mself\u001b[39m.dataset.source_type\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mself\u001b[39m.source_type.stream\n\u001b[32m    269\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.screenshot\n\u001b[32m    270\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset) > \u001b[32m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[32m    271\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33mvideo_flag\u001b[39m\u001b[33m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[32m    272\u001b[39m ):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl_project/venv/lib/python3.12/site-packages/ultralytics/data/build.py:294\u001b[39m, in \u001b[36mload_inference_source\u001b[39m\u001b[34m(source, batch, vid_stride, buffer, channels)\u001b[39m\n\u001b[32m    292\u001b[39m     dataset = source\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     dataset = \u001b[43mLoadStreams\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m screenshot:\n\u001b[32m    296\u001b[39m     dataset = LoadScreenshots(source, channels=channels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl_project/venv/lib/python3.12/site-packages/ultralytics/data/loaders.py:147\u001b[39m, in \u001b[36mLoadStreams.__init__\u001b[39m\u001b[34m(self, sources, vid_stride, buffer, channels)\u001b[39m\n\u001b[32m    145\u001b[39m im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)[..., \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2_flag == cv2.IMREAD_GRAYSCALE \u001b[38;5;28;01melse\u001b[39;00m im\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success \u001b[38;5;129;01mor\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mst\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mFailed to read images from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    148\u001b[39m \u001b[38;5;28mself\u001b[39m.imgs[i].append(im)\n\u001b[32m    149\u001b[39m \u001b[38;5;28mself\u001b[39m.shape[i] = im.shape\n",
      "\u001b[31mConnectionError\u001b[39m: 1/1: 0... Failed to read images from 0"
     ]
    }
   ],
   "source": [
    "results = model.predict(source=\"dataset_split/images/test\", conf=0.4, save=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
