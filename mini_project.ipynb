{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97869d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.12/site-packages (25.2)\n",
      "Requirement already satisfied: ultralytics in ./venv/lib/python3.12/site-packages (8.3.174)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy>=1.23.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (3.10.5)\n",
      "Requirement already satisfied: pillow>=7.1.2 in ./venv/lib/python3.12/site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./venv/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./venv/lib/python3.12/site-packages (from ultralytics) (1.16.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.7.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.12/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in ./venv/lib/python3.12/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.3.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in ./venv/lib/python3.12/site-packages (from ultralytics) (2.0.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2025.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# to use virtual env\n",
    "# python3 -m venv venv\n",
    "# source venv/bin/activate\n",
    "\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install ultralytics opencv-python\n",
    "!pip3 install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04189ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset: https://www.kaggle.com/datasets/andrewmvd/car-plate-detection?resource=download\n",
    "# rename the folder: \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2283c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4155ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files split, grouped, and labels converted to YOLO TXT format\n"
     ]
    }
   ],
   "source": [
    "# process the annotations in YOLO text format\n",
    "# split the data into train 70%, val 20% and test 10%\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "image_dir = 'dataset/images'\n",
    "label_dir = 'dataset/annotations'\n",
    "\n",
    "out_base = 'dataset_split'\n",
    "\n",
    "def convert_xml_to_yolo(xml_path, txt_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    size = root.find('size')\n",
    "    width = float(size.find('width').text)\n",
    "    height = float(size.find('height').text)\n",
    "\n",
    "    lines = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_id = 0 \n",
    "\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = float(bndbox.find('xmin').text)\n",
    "        ymin = float(bndbox.find('ymin').text)\n",
    "        xmax = float(bndbox.find('xmax').text)\n",
    "        ymax = float(bndbox.find('ymax').text)\n",
    "\n",
    "        x_center = ((xmin + xmax) / 2) / width\n",
    "        y_center = ((ymin + ymax) / 2) / height\n",
    "        w = (xmax - xmin) / width\n",
    "        h = (ymax - ymin) / height\n",
    "\n",
    "        lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\")\n",
    "\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "base_names = [os.path.splitext(f)[0] for f in image_files]\n",
    "\n",
    "random.shuffle(base_names)\n",
    "total = len(base_names)\n",
    "train_end = int(0.7 * total)\n",
    "val_end = int(0.9 * total)\n",
    "\n",
    "splits = {\n",
    "    'train': base_names[:train_end],\n",
    "    'val': base_names[train_end:val_end],\n",
    "    'test': base_names[val_end:]\n",
    "}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(f'{out_base}/images/{split}', exist_ok=True)\n",
    "    os.makedirs(f'{out_base}/labels/{split}', exist_ok=True)\n",
    "\n",
    "for split, names in splits.items():\n",
    "    for name in names:\n",
    "        img_src = os.path.join(image_dir, f'{name}.png')\n",
    "        lbl_src = os.path.join(label_dir, f'{name}.xml')\n",
    "\n",
    "        img_dst = os.path.join(out_base, 'images', split, f'{name}.png')\n",
    "        txt_dst = os.path.join(out_base, 'labels', split, f'{name}.txt')\n",
    "\n",
    "        if os.path.exists(img_src):\n",
    "            shutil.copy(img_src, img_dst)\n",
    "\n",
    "        if os.path.exists(lbl_src):\n",
    "            convert_xml_to_yolo(lbl_src, txt_dst)\n",
    "\n",
    "print(\"Files split, grouped, and labels converted to YOLO TXT format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34953ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "080e50df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training yolov8n.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_split/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train8, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/neslyn/Desktop/dl_project/runs/detect/train8, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1015.3Â±200.0 MB/s, size: 575.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/train.cache... 303 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 794.7Â±305.8 MB/s, size: 559.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/neslyn/Desktop/dl_project/runs/detect/train8/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train8\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.462      3.249      1.354         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:57<00:00,  9.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92    0.00349      0.978      0.577       0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.416      2.132      1.169         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:47<00:00,  8.84s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.526      0.435      0.416      0.203\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/5         0G      1.433      1.816      1.179         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:39<00:00,  8.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.933      0.152      0.441      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/5         0G      1.349       1.63      1.167         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:43<00:00,  8.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.907      0.422      0.635      0.343\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        5/5         0G      1.337       1.62      1.172         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:51<00:00,  9.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92        0.9      0.586      0.764      0.422\n",
      "\n",
      "5 epochs completed in 0.256 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train8/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train8/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /Users/neslyn/Desktop/dl_project/runs/detect/train8/weights/best.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92        0.9      0.585      0.764      0.422\n",
      "Speed: 1.1ms preprocess, 170.1ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train8\u001b[0m\n",
      "\n",
      "Training yolov8s.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_split/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train9, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/neslyn/Desktop/dl_project/runs/detect/train9, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1237.8Â±1626.2 MB/s, size: 575.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/train.cache... 303 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2747.7Â±1552.6 MB/s, size: 559.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/neslyn/Desktop/dl_project/runs/detect/train9/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train9\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.523      3.617      1.331         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:55<00:00, 21.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.775      0.788      0.794      0.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.491      1.484      1.253         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [07:54<00:00, 25.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:39<00:00, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.796      0.641      0.681      0.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.475      1.157      1.266         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [07:17<00:00, 23.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:39<00:00, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.808      0.609      0.726      0.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.365      1.001      1.199         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:52<00:00, 21.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:36<00:00, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92       0.93      0.723      0.838      0.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G       1.34     0.9606      1.231         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:25<00:00, 20.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:34<00:00, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.644       0.67      0.659      0.319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.644 hours.\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train9/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train9/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /Users/neslyn/Desktop/dl_project/runs/detect/train9/weights/best.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:32<00:00, 10.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92       0.93      0.724      0.839      0.457\n",
      "Speed: 0.7ms preprocess, 367.6ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train9\u001b[0m\n",
      "\n",
      "Training yolov8m.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_split/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train10, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/neslyn/Desktop/dl_project/runs/detect/train10, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 169 layers, 25,856,899 parameters, 25,856,883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 869.2Â±364.6 MB/s, size: 575.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/train.cache... 303 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5575.8Â±1331.7 MB/s, size: 559.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/neslyn/Desktop/dl_project/runs/detect/train10/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train10\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.541      2.349      1.367         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [15:19<00:00, 48.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:15<00:00, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.335      0.533      0.329      0.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.556      1.453      1.324         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [15:11<00:00, 47.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:10<00:00, 23.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92    0.00975      0.674    0.00863    0.00455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.506      1.139      1.347         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [14:36<00:00, 46.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:13<00:00, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92        0.8      0.728      0.738      0.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.442      1.073      1.275         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [14:57<00:00, 47.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:08<00:00, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92    0.00192      0.326    0.00124   0.000547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.401      1.027      1.299         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [15:42<00:00, 49.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:15<00:00, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.693      0.467       0.47      0.236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 1.365 hours.\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train10/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train10/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating /Users/neslyn/Desktop/dl_project/runs/detect/train10/weights/best.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 92 layers, 25,840,339 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:11<00:00, 23.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92        0.8      0.728      0.739      0.384\n",
      "Speed: 0.9ms preprocess, 819.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train10\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>mAP50</th>\n",
       "      <th>Training Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yolov8n.pt</td>\n",
       "      <td>0.8997</td>\n",
       "      <td>0.5853</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7642</td>\n",
       "      <td>941.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yolov8s.pt</td>\n",
       "      <td>0.9301</td>\n",
       "      <td>0.7235</td>\n",
       "      <td>None</td>\n",
       "      <td>0.8386</td>\n",
       "      <td>2354.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yolov8m.pt</td>\n",
       "      <td>0.8001</td>\n",
       "      <td>0.7283</td>\n",
       "      <td>None</td>\n",
       "      <td>0.7388</td>\n",
       "      <td>4992.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Precision  Recall F1 Score   mAP50  Training Time (s)\n",
       "0  yolov8n.pt     0.8997  0.5853     None  0.7642             941.91\n",
       "1  yolov8s.pt     0.9301  0.7235     None  0.8386            2354.00\n",
       "2  yolov8m.pt     0.8001  0.7283     None  0.7388            4992.39"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\"yolov8n.pt\", \"yolov8s.pt\", \"yolov8m.pt\"]\n",
    "data_yaml = \"dataset_split/data.yaml\"\n",
    "epochs = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model = YOLO(model_name)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    metrics = model.train(data=data_yaml, epochs=epochs)\n",
    "    end_time = time.time()\n",
    "    train_time = round(end_time - start_time, 2)\n",
    "    \n",
    "    map50 = metrics.results_dict.get(\"metrics/mAP50(B)\", None)\n",
    "    recall = metrics.results_dict.get(\"metrics/recall(B)\", None)\n",
    "    precision = metrics.results_dict.get(\"metrics/precision(B)\", None)\n",
    "    f1 = metrics.results_dict.get(\"metrics/f1(B)\", None)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision\": round(precision, 4) if precision else None,\n",
    "        \"Recall\": round(recall, 4) if recall else None,\n",
    "        \"F1 Score\": round(f1, 4) if f1 else None,\n",
    "        \"mAP50\": round(map50, 4) if map50 else None,\n",
    "        \"Training Time (s)\": train_time\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"yolo_model_comparison.csv\", index=False)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df750a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_split/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train13, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/neslyn/Desktop/dl_project/runs/detect/train13, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.1Â±0.1 ms, read: 923.8Â±120.7 MB/s, size: 575.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/train.cache... 303 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.1 ms, read: 687.5Â±137.3 MB/s, size: 559.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/neslyn/Desktop/dl_project/runs/detect/train13/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train13\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G       1.66      5.573      1.374         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:49<00:00, 18.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.908      0.565      0.705      0.376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G       1.54      1.733      1.309         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:59<00:00, 18.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.517      0.652      0.595      0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.612      1.554      1.376         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [05:35<00:00, 17.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:33<00:00, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.495      0.554      0.543        0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.531       1.36      1.379         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [07:05<00:00, 22.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:37<00:00, 12.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.791       0.62      0.694      0.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.587      1.238      1.359         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:34<00:00, 20.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:34<00:00, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.599      0.617      0.616      0.276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.527      1.133      1.371         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:38<00:00, 20.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:35<00:00, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.677       0.57      0.602      0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.494      1.061      1.337         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:47<00:00, 21.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:38<00:00, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92        0.8      0.696      0.721      0.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.461     0.9745      1.347         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:45<00:00, 21.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:37<00:00, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.914      0.772      0.871      0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.385      0.887      1.292         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [06:55<00:00, 21.88s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:40<00:00, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92       0.86      0.783      0.894      0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.366     0.8527      1.277         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [07:39<00:00, 24.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:42<00:00, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.895      0.859       0.91      0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 1.205 hours.\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train13/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /Users/neslyn/Desktop/dl_project/runs/detect/train13/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /Users/neslyn/Desktop/dl_project/runs/detect/train13/weights/best.pt...\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:42<00:00, 14.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.894      0.859       0.91      0.543\n",
      "Speed: 1.0ms preprocess, 478.0ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train13\u001b[0m\n",
      "Ultralytics 8.3.174 ðŸš€ Python-3.12.8 torch-2.7.1 CPU (Apple M1)\n",
      "Model summary (fused): 72 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 598.5Â±203.9 MB/s, size: 307.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/neslyn/Desktop/dl_project/dataset_split/labels/val.cache... 86 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:40<00:00,  6.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         86         92      0.892      0.848       0.91      0.546\n",
      "Speed: 0.7ms preprocess, 447.8ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/neslyn/Desktop/dl_project/runs/detect/train132\u001b[0m\n",
      "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
      "\n",
      "ap_class_index: array([0])\n",
      "box: ultralytics.utils.metrics.Metric object\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x11068cbf0>\n",
      "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
      "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,\n",
      "            0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,\n",
      "            0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,\n",
      "            0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,\n",
      "            0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.98246,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,\n",
      "            0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.96721,     0.94203,     0.94203,     0.94203,\n",
      "            0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,\n",
      "            0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,\n",
      "            0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.94203,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,\n",
      "            0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,\n",
      "            0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,     0.93333,\n",
      "            0.93333,     0.93333,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,\n",
      "            0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,\n",
      "            0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.91463,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,\n",
      "            0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.90698,     0.88764,     0.88764,     0.88764,     0.88764,\n",
      "            0.88764,     0.88764,     0.88764,     0.88764,     0.88764,     0.88764,     0.88764,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,\n",
      "            0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.84536,     0.82178,     0.82178,     0.82178,     0.82178,     0.82178,     0.82178,\n",
      "            0.82178,     0.82178,     0.82178,     0.82178,     0.82178,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,     0.81731,\n",
      "            0.81731,     0.81731,     0.81731,     0.77477,     0.77477,     0.77477,     0.77477,     0.77477,     0.77477,     0.77477,     0.77477,     0.77477,     0.77477,     0.77477,     0.26444,     0.26444,     0.26444,     0.26444,     0.26444,     0.26444,     0.26444,     0.26444,     0.26444,\n",
      "            0.26444,     0.26444,     0.20418,     0.20418,     0.20418,     0.20418,     0.20418,     0.20418,     0.20418,     0.20418,     0.20418,     0.20418,     0.20418,      0.1092,      0.1092,      0.1092,      0.1092,      0.1092,      0.1092,      0.1092,      0.1092,      0.1092,      0.1092,\n",
      "             0.1092,    0.048651,    0.047131,     0.04561,     0.04409,     0.04257,    0.041049,    0.039529,    0.038009,    0.036488,    0.034968,    0.033448,    0.031927,    0.030407,    0.028887,    0.027366,    0.025846,    0.024326,    0.022805,    0.021285,    0.019765,    0.018244,    0.016724,\n",
      "           0.015203,    0.013683,    0.012163,    0.010642,   0.0091221,   0.0076017,   0.0060814,    0.004561,   0.0030407,   0.0015203,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.09423,     0.09423,      0.1729,     0.22177,     0.26411,     0.28765,     0.31337,     0.33423,     0.35276,      0.3654,     0.38322,     0.40174,     0.41317,     0.43174,     0.44756,     0.45775,     0.46469,     0.47627,     0.48334,     0.48971,     0.49566,     0.50561,     0.51149,\n",
      "            0.52295,     0.52692,     0.53083,     0.53666,     0.54533,     0.55215,      0.5625,     0.56976,     0.57446,     0.57607,     0.57713,     0.58133,     0.58321,     0.58398,     0.58476,      0.5921,     0.59865,     0.60088,     0.60183,     0.60236,      0.6029,     0.60343,     0.60732,\n",
      "            0.60857,     0.60955,      0.6121,     0.61315,     0.61419,     0.61896,     0.62005,     0.62186,     0.62621,     0.62721,     0.62957,      0.6364,     0.64703,     0.64804,     0.64904,     0.66666,     0.67176,     0.67435,     0.68217,     0.68693,     0.69174,     0.69369,     0.69529,\n",
      "            0.69651,     0.69698,     0.69745,     0.69792,     0.69839,     0.69885,     0.70061,     0.70285,       0.704,     0.70557,     0.70804,     0.70875,     0.70945,     0.71015,     0.71104,     0.71303,     0.71851,     0.72022,     0.72115,     0.72208,     0.72319,     0.72466,      0.7258,\n",
      "            0.72603,     0.72626,     0.72649,     0.72672,     0.72696,     0.72719,     0.72742,     0.72765,     0.72788,     0.72811,     0.72834,     0.72857,      0.7288,     0.73223,     0.73261,     0.73298,     0.73335,     0.73372,     0.73409,     0.73446,     0.73483,      0.7354,     0.73623,\n",
      "            0.73705,     0.73787,     0.73837,     0.73865,     0.73893,     0.73921,     0.73949,     0.73978,     0.74006,     0.74034,     0.74062,      0.7409,     0.74118,     0.74161,     0.74239,     0.74318,     0.74396,     0.74465,     0.74494,     0.74524,     0.74554,     0.74583,     0.74613,\n",
      "            0.74642,     0.74672,     0.74701,     0.74731,      0.7476,     0.74827,     0.75017,     0.75349,     0.75785,     0.75851,     0.75917,     0.75983,     0.76049,     0.76454,     0.76525,     0.76595,     0.76666,     0.76737,     0.76801,     0.76849,     0.76897,     0.76945,     0.76993,\n",
      "            0.77041,     0.77089,     0.77483,     0.77524,     0.77565,     0.77606,     0.77647,     0.77687,     0.77728,     0.77769,     0.77809,     0.77859,     0.77916,     0.77974,     0.78031,     0.78088,     0.78145,     0.78248,     0.78435,     0.78762,     0.79285,     0.79325,     0.79365,\n",
      "            0.79404,     0.79444,     0.79484,     0.79523,     0.79563,     0.79603,     0.79705,     0.79939,     0.80087,     0.80204,     0.80321,     0.80601,     0.81203,     0.81314,     0.81425,     0.81725,     0.82101,       0.825,     0.82745,     0.82807,     0.82869,      0.8293,     0.82992,\n",
      "            0.83053,     0.83983,     0.84201,     0.84341,     0.84397,     0.84453,      0.8451,     0.84566,     0.84622,     0.84678,     0.84644,     0.84204,     0.84289,     0.84374,     0.84459,     0.84544,     0.84628,     0.84713,     0.84798,     0.84882,     0.84966,     0.85878,     0.85911,\n",
      "            0.85944,     0.85977,      0.8601,     0.86043,     0.86075,     0.86108,     0.86141,     0.86174,     0.86206,     0.86239,     0.86272,     0.86344,     0.86507,     0.86669,      0.8672,     0.86694,     0.86669,     0.86643,     0.86618,     0.86592,     0.86566,     0.86541,     0.86515,\n",
      "             0.8649,     0.86464,     0.86439,     0.86413,     0.86387,     0.86362,     0.86336,      0.8631,     0.86285,     0.86259,     0.86233,     0.86208,     0.86182,     0.86156,      0.8582,     0.85579,     0.85616,     0.85652,     0.85689,     0.85725,     0.85762,     0.85798,     0.85835,\n",
      "            0.85871,     0.85908,     0.85944,      0.8598,     0.86006,     0.85984,     0.85962,      0.8594,     0.85917,     0.85895,     0.85873,      0.8585,     0.85828,     0.85806,     0.85783,     0.85761,     0.85739,     0.85716,     0.85694,     0.85672,     0.85649,     0.85627,     0.85605,\n",
      "            0.85582,      0.8556,     0.85538,     0.85515,     0.85493,      0.8547,     0.85448,     0.85425,     0.85495,     0.85625,     0.85754,     0.85879,     0.85981,     0.86083,     0.86185,     0.86287,     0.86499,     0.86753,     0.86416,     0.86136,     0.86044,     0.85952,      0.8586,\n",
      "            0.85768,     0.85675,     0.85583,      0.8564,     0.85743,     0.85845,     0.85947,      0.8603,     0.86062,     0.86093,     0.86125,     0.86156,     0.86188,      0.8622,     0.86251,     0.86283,     0.86314,     0.86346,     0.86377,     0.86408,      0.8644,     0.86471,     0.86501,\n",
      "            0.86528,     0.86556,     0.86583,     0.86611,     0.86638,     0.86666,     0.86693,     0.86721,     0.86748,     0.86775,     0.86803,      0.8683,     0.86857,     0.86885,     0.86912,     0.86939,     0.86908,     0.86778,     0.86647,     0.86516,     0.86385,     0.86369,     0.86416,\n",
      "            0.86462,     0.86508,     0.86555,     0.86601,     0.86647,     0.86693,     0.86739,     0.86785,     0.87202,     0.87111,      0.8691,     0.86709,     0.86695,     0.86731,     0.86768,     0.86804,      0.8684,     0.86876,     0.86912,     0.86948,     0.86984,      0.8702,     0.87056,\n",
      "            0.87092,     0.87127,     0.87155,     0.87167,     0.87178,      0.8719,     0.87202,     0.87213,     0.87225,     0.87237,     0.87249,      0.8726,     0.87272,     0.87284,     0.87295,     0.87307,     0.87319,      0.8733,     0.87342,     0.87354,     0.87365,     0.87377,     0.87389,\n",
      "              0.874,     0.87412,     0.87424,     0.87435,     0.87447,     0.87459,      0.8747,     0.87482,     0.87494,     0.87505,     0.87517,     0.87528,      0.8754,     0.87552,     0.87563,     0.87575,     0.87586,     0.87598,      0.8761,     0.87621,     0.87633,     0.87542,     0.87257,\n",
      "            0.86349,     0.86227,     0.86105,     0.85983,     0.85861,     0.85739,     0.86037,     0.86093,     0.85894,     0.85695,     0.85526,     0.85441,     0.85356,     0.85271,     0.85185,       0.851,     0.85014,     0.84929,     0.84484,     0.84179,     0.84131,     0.84084,     0.84036,\n",
      "            0.83989,     0.83941,     0.83894,     0.83846,     0.83799,     0.83751,     0.83703,     0.83656,     0.83608,      0.8356,     0.83524,     0.83507,      0.8349,     0.83474,     0.83457,      0.8344,     0.83424,     0.83407,      0.8339,     0.83374,     0.83357,      0.8334,     0.83324,\n",
      "            0.83307,      0.8329,     0.83274,     0.83257,      0.8324,     0.83223,     0.83207,      0.8319,     0.83173,     0.83157,      0.8314,     0.83123,     0.83106,      0.8309,     0.83073,     0.83056,     0.83039,     0.83023,     0.83006,     0.82989,     0.82972,     0.82956,     0.82939,\n",
      "            0.82922,     0.82905,     0.82888,     0.82872,     0.82855,     0.82865,      0.8305,     0.83233,     0.83347,     0.83377,     0.83407,     0.83436,     0.83466,     0.83496,     0.83526,     0.83555,     0.83585,     0.83615,     0.83644,     0.83674,     0.83704,     0.83733,     0.83763,\n",
      "            0.83792,     0.83822,     0.83806,     0.83766,     0.83726,     0.83686,     0.83645,     0.83605,     0.83565,     0.83524,     0.83484,     0.83443,     0.83403,     0.83362,     0.83322,     0.83281,     0.83241,       0.832,      0.8316,     0.81622,     0.81368,     0.81112,     0.80931,\n",
      "            0.80827,     0.80722,     0.80618,     0.80513,     0.80408,     0.80304,     0.80259,     0.80286,     0.80313,     0.80339,     0.80366,     0.80393,     0.80419,     0.80446,     0.80472,     0.80499,     0.80525,     0.80552,     0.80578,     0.80605,     0.80631,     0.80657,     0.80684,\n",
      "             0.8071,     0.80736,     0.80726,     0.80695,     0.80665,     0.80635,     0.80605,     0.80575,     0.80545,     0.80515,     0.80484,     0.80454,     0.80424,     0.80394,     0.80364,     0.80333,     0.80303,     0.80273,     0.80242,     0.80212,     0.80182,     0.80151,     0.80121,\n",
      "            0.80091,      0.8006,      0.8003,     0.79243,     0.79094,     0.78945,     0.78796,     0.78646,     0.78496,     0.78404,     0.78318,     0.78232,     0.78147,     0.78061,     0.77975,     0.77888,     0.77802,     0.77716,     0.77638,     0.77561,     0.77484,     0.77407,      0.7733,\n",
      "            0.77253,     0.77176,     0.77098,     0.77021,     0.76943,     0.76903,     0.76875,     0.76848,      0.7682,     0.76793,     0.76765,     0.76738,      0.7671,     0.76683,     0.76655,     0.76627,       0.766,     0.76572,     0.76545,     0.76517,     0.76489,     0.76462,     0.76434,\n",
      "            0.76406,     0.76379,     0.76351,     0.76323,     0.76296,     0.76268,      0.7624,     0.76212,     0.76185,     0.76157,     0.76129,     0.76564,     0.76652,     0.76684,     0.76717,     0.76749,     0.76782,     0.76814,     0.76847,     0.76879,     0.76912,     0.76944,     0.76976,\n",
      "            0.77008,     0.77041,     0.77073,     0.77105,     0.77082,     0.76977,     0.76872,     0.76767,     0.76662,     0.76556,     0.76451,     0.76345,     0.76191,     0.76018,     0.75845,     0.75671,     0.75497,      0.7541,     0.75324,     0.75238,     0.75151,     0.75065,     0.74978,\n",
      "            0.74891,     0.74804,     0.74717,     0.75129,     0.75038,     0.74946,     0.74855,     0.74763,     0.74671,     0.74579,     0.74487,     0.74395,     0.73368,     0.72685,      0.7254,     0.72469,     0.72397,     0.72326,     0.72254,     0.72183,     0.72111,     0.72039,     0.71968,\n",
      "            0.71896,     0.71824,     0.71752,     0.71617,     0.71444,     0.71271,     0.71098,     0.70924,     0.69003,     0.68934,     0.68864,     0.68794,     0.68725,     0.68655,     0.68585,     0.68515,     0.68445,     0.68374,     0.68304,     0.68234,     0.68163,     0.68093,      0.6812,\n",
      "            0.68159,     0.68198,     0.68236,     0.68275,     0.68314,     0.68352,     0.68391,     0.68429,     0.68467,     0.68506,     0.68544,     0.68549,     0.68468,     0.68386,     0.68304,     0.68222,      0.6814,     0.68058,     0.67976,     0.67893,     0.67811,     0.67728,     0.67646,\n",
      "            0.67485,     0.67299,     0.67113,     0.66926,     0.66738,     0.65923,     0.65531,     0.65329,     0.65128,     0.64925,     0.64722,     0.63985,     0.62999,     0.62611,     0.62506,     0.62401,     0.62295,      0.6219,     0.62084,     0.61978,     0.61872,     0.61766,     0.61659,\n",
      "            0.60446,     0.59767,     0.59181,     0.58638,     0.58077,     0.57503,     0.57192,     0.56965,     0.56736,     0.56507,     0.56278,     0.56127,     0.55986,     0.55845,     0.55704,     0.55563,     0.55421,     0.55279,     0.55137,      0.5326,     0.52724,     0.52559,     0.52394,\n",
      "            0.52228,     0.52063,     0.51896,      0.5173,     0.51519,     0.51206,     0.50892,     0.50576,     0.50099,     0.49431,     0.48715,     0.47966,     0.47454,      0.4695,     0.46517,     0.46175,     0.45832,     0.45488,     0.43921,     0.43706,      0.4349,     0.43273,     0.43056,\n",
      "            0.42838,     0.42203,     0.41338,     0.41123,     0.40908,     0.40692,     0.40475,     0.40258,      0.4004,     0.39774,     0.39497,     0.39219,      0.3894,      0.3866,     0.37846,     0.37076,     0.36775,     0.36474,     0.36171,     0.35868,     0.32638,      0.3204,     0.31438,\n",
      "             0.3006,     0.29514,     0.29364,     0.29214,     0.29064,     0.28914,     0.28763,     0.28613,     0.28461,      0.2831,     0.28158,     0.27874,     0.27068,     0.24714,     0.24445,     0.24176,     0.23905,     0.23634,     0.23361,     0.23088,     0.22906,     0.22727,     0.22548,\n",
      "            0.22368,     0.22188,     0.22008,     0.21827,     0.21646,     0.21465,      0.2014,      0.1752,     0.17151,     0.16779,     0.16407,     0.16033,     0.15637,     0.15237,     0.14836,     0.14433,     0.13159,     0.12151,     0.12025,     0.11898,     0.11771,     0.11644,     0.11517,\n",
      "             0.1139,     0.11262,     0.11134,     0.11007,     0.10878,      0.1075,     0.10622,     0.10493,     0.10365,     0.10111,    0.097618,    0.094117,    0.090603,    0.087076,    0.083536,    0.049527,    0.038511,     0.03166,    0.024761,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.049527,    0.049527,    0.094934,     0.12543,      0.1532,     0.16928,     0.18738,     0.20249,     0.21682,     0.22645,      0.2403,     0.25505,     0.26433,     0.28069,     0.29421,     0.30308,      0.3092,     0.31953,     0.32593,     0.33175,     0.33724,     0.34651,     0.35207,\n",
      "            0.36302,     0.36685,     0.37066,     0.37637,     0.38495,     0.39179,     0.40228,     0.40975,     0.41464,     0.41632,     0.41742,     0.42183,     0.42381,     0.42463,     0.42545,     0.43327,     0.44032,     0.44273,     0.44376,     0.44435,     0.44493,     0.44551,     0.44976,\n",
      "            0.45114,     0.45221,     0.45503,     0.45619,     0.45734,     0.46266,     0.46386,      0.4659,      0.4708,     0.47194,     0.47461,     0.48241,     0.49474,     0.49592,     0.49709,     0.51807,     0.52425,     0.52741,     0.53704,     0.54296,       0.549,     0.55147,     0.55348,\n",
      "            0.55503,     0.55563,     0.55623,     0.55682,     0.55742,     0.55802,     0.56026,     0.56313,     0.56461,     0.56663,     0.56983,     0.57074,     0.57165,     0.57256,     0.57372,     0.57631,     0.58351,     0.58577,       0.587,     0.58823,     0.58971,     0.59166,     0.59318,\n",
      "            0.59349,      0.5938,     0.59411,     0.59442,     0.59473,     0.59504,     0.59535,     0.59566,     0.59597,     0.59628,     0.59659,      0.5969,      0.5972,     0.60183,     0.60233,     0.60284,     0.60334,     0.60384,     0.60434,     0.60485,     0.60535,     0.60613,     0.60725,\n",
      "            0.60837,     0.60949,     0.61016,     0.61055,     0.61093,     0.61132,      0.6117,     0.61209,     0.61247,     0.61286,     0.61324,     0.61363,     0.61401,      0.6146,     0.61568,     0.61676,     0.61784,     0.61879,      0.6192,      0.6196,     0.62001,     0.62042,     0.62083,\n",
      "            0.62124,     0.62165,     0.62206,     0.62247,     0.62287,     0.62381,     0.62645,      0.6311,     0.63723,     0.63817,      0.6391,     0.64004,     0.64097,     0.64675,     0.64776,     0.64878,      0.6498,     0.65081,     0.65173,     0.65242,     0.65312,     0.65381,     0.65451,\n",
      "             0.6552,     0.65589,     0.66162,     0.66222,     0.66282,     0.66341,     0.66401,      0.6646,      0.6652,      0.6658,     0.66639,     0.66712,     0.66796,      0.6688,     0.66965,     0.67049,     0.67133,     0.67285,     0.67563,     0.68049,     0.68834,     0.68894,     0.68954,\n",
      "            0.69014,     0.69074,     0.69134,     0.69194,     0.69254,     0.69314,     0.69469,     0.69825,     0.70051,     0.70231,     0.70411,     0.70841,     0.71777,     0.71951,     0.72125,     0.72598,     0.73193,      0.7383,     0.74223,     0.74323,     0.74422,     0.74521,     0.74621,\n",
      "             0.7472,     0.76239,     0.76599,     0.76831,     0.76924,     0.77018,     0.77111,     0.77205,     0.77298,     0.77392,     0.77447,     0.77349,     0.77493,     0.77637,     0.77781,     0.77925,     0.78069,     0.78213,     0.78357,     0.78502,     0.78646,     0.80223,      0.8028,\n",
      "            0.80338,     0.80395,     0.80453,      0.8051,     0.80568,     0.80625,     0.80683,      0.8074,     0.80798,     0.80855,     0.80912,      0.8104,     0.81327,     0.81613,     0.81726,     0.81718,     0.81711,     0.81703,     0.81695,     0.81687,     0.81679,     0.81671,     0.81664,\n",
      "            0.81656,     0.81648,      0.8164,     0.81632,     0.81625,     0.81617,     0.81609,     0.81601,     0.81593,     0.81585,     0.81578,      0.8157,     0.81562,     0.81554,      0.8145,     0.81394,      0.8146,     0.81527,     0.81593,     0.81659,     0.81726,     0.81792,     0.81858,\n",
      "            0.81925,     0.81991,     0.82057,     0.82124,     0.82177,      0.8217,     0.82164,     0.82157,      0.8215,     0.82143,     0.82137,      0.8213,     0.82123,     0.82117,      0.8211,     0.82103,     0.82097,      0.8209,     0.82083,     0.82076,      0.8207,     0.82063,     0.82056,\n",
      "             0.8205,     0.82043,     0.82036,     0.82029,     0.82023,     0.82016,     0.82009,     0.82003,     0.82145,     0.82384,     0.82623,     0.82856,     0.83047,     0.83238,     0.83429,     0.83619,     0.84018,     0.84499,     0.84441,     0.84366,     0.84341,     0.84316,     0.84291,\n",
      "            0.84266,     0.84241,     0.84216,     0.84364,     0.84563,     0.84762,     0.84961,     0.85123,     0.85185,     0.85247,     0.85309,     0.85371,     0.85433,     0.85495,     0.85557,     0.85619,     0.85681,     0.85743,     0.85805,     0.85867,     0.85929,     0.85991,     0.86049,\n",
      "            0.86104,     0.86159,     0.86213,     0.86268,     0.86322,     0.86377,     0.86431,     0.86486,      0.8654,     0.86595,      0.8665,     0.86704,     0.86759,     0.86813,     0.86868,     0.86922,     0.86945,     0.86915,     0.86885,     0.86854,     0.86824,     0.86874,     0.86968,\n",
      "            0.87063,     0.87157,     0.87251,     0.87345,     0.87439,     0.87533,     0.87627,     0.87721,     0.88577,     0.88727,     0.88686,     0.88645,     0.88696,     0.88772,     0.88848,     0.88924,     0.88999,     0.89075,     0.89151,     0.89227,     0.89302,     0.89378,     0.89454,\n",
      "             0.8953,     0.89606,     0.89664,     0.89689,     0.89713,     0.89738,     0.89763,     0.89788,     0.89813,     0.89837,     0.89862,     0.89887,     0.89912,     0.89937,     0.89962,     0.89986,     0.90011,     0.90036,     0.90061,     0.90086,     0.90111,     0.90135,      0.9016,\n",
      "            0.90185,      0.9021,     0.90235,     0.90259,     0.90284,     0.90309,     0.90334,     0.90359,     0.90384,     0.90408,     0.90433,     0.90458,     0.90483,     0.90508,     0.90533,     0.90557,     0.90582,     0.90607,     0.90632,     0.90657,     0.90681,     0.90681,     0.90631,\n",
      "            0.90474,     0.90452,      0.9043,     0.90409,     0.90387,     0.90366,     0.91082,     0.91445,     0.91413,     0.91381,     0.91354,      0.9134,     0.91327,     0.91313,     0.91299,     0.91285,     0.91271,     0.91257,     0.91184,     0.91134,     0.91126,     0.91118,      0.9111,\n",
      "            0.91102,     0.91094,     0.91086,     0.91078,      0.9107,     0.91062,     0.91055,     0.91047,     0.91039,     0.91031,     0.91025,     0.91022,     0.91019,     0.91016,     0.91013,     0.91011,     0.91008,     0.91005,     0.91002,     0.90999,     0.90996,     0.90994,     0.90991,\n",
      "            0.90988,     0.90985,     0.90982,     0.90979,     0.90977,     0.90974,     0.90971,     0.90968,     0.90965,     0.90962,      0.9096,     0.90957,     0.90954,     0.90951,     0.90948,     0.90945,     0.90943,      0.9094,     0.90937,     0.90934,     0.90931,     0.90929,     0.90926,\n",
      "            0.90923,      0.9092,     0.90917,     0.90914,     0.90912,     0.90969,     0.91415,     0.91861,     0.92138,     0.92211,     0.92285,     0.92358,     0.92431,     0.92504,     0.92577,      0.9265,     0.92723,     0.92796,     0.92869,     0.92942,     0.93015,     0.93088,     0.93161,\n",
      "            0.93234,     0.93307,      0.9333,     0.93325,      0.9332,     0.93314,     0.93309,     0.93304,     0.93299,     0.93294,     0.93288,     0.93283,     0.93278,     0.93273,     0.93268,     0.93262,     0.93257,     0.93252,     0.93247,     0.93044,      0.9301,     0.92975,     0.92951,\n",
      "            0.92936,     0.92922,     0.92908,     0.92893,     0.92879,     0.92865,      0.9289,     0.92962,     0.93033,     0.93105,     0.93176,     0.93248,      0.9332,     0.93391,     0.93463,     0.93534,     0.93606,     0.93677,     0.93749,     0.93821,     0.93892,     0.93964,     0.94035,\n",
      "            0.94107,     0.94179,     0.94201,     0.94197,     0.94194,      0.9419,     0.94187,     0.94183,      0.9418,     0.94176,     0.94173,     0.94169,     0.94166,     0.94163,     0.94159,     0.94156,     0.94152,     0.94149,     0.94145,     0.94142,     0.94138,     0.94135,     0.94131,\n",
      "            0.94128,     0.94125,     0.94121,      0.9403,     0.94012,     0.93994,     0.93977,     0.93959,     0.93941,      0.9393,      0.9392,     0.93909,     0.93899,     0.93889,     0.93878,     0.93868,     0.93858,     0.93847,     0.93838,     0.93828,     0.93819,     0.93809,       0.938,\n",
      "             0.9379,     0.93781,     0.93771,     0.93762,     0.93752,     0.93747,     0.93744,     0.93741,     0.93737,     0.93734,      0.9373,     0.93727,     0.93723,      0.9372,     0.93716,     0.93713,     0.93709,     0.93706,     0.93703,     0.93699,     0.93696,     0.93692,     0.93689,\n",
      "            0.93685,     0.93682,     0.93678,     0.93675,     0.93672,     0.93668,     0.93665,     0.93661,     0.93658,     0.93654,     0.93651,     0.94979,     0.95248,     0.95349,      0.9545,     0.95551,     0.95652,     0.95753,     0.95854,     0.95955,     0.96056,     0.96156,     0.96257,\n",
      "            0.96358,     0.96459,      0.9656,     0.96661,     0.96718,     0.96711,     0.96704,     0.96697,      0.9669,     0.96683,     0.96676,     0.96669,     0.96658,     0.96646,     0.96634,     0.96622,      0.9661,     0.96604,     0.96598,     0.96592,     0.96586,      0.9658,     0.96574,\n",
      "            0.96567,     0.96561,     0.96555,     0.98244,     0.98241,     0.98237,     0.98234,     0.98231,     0.98227,     0.98224,      0.9822,     0.98217,     0.98178,     0.98151,     0.98146,     0.98143,      0.9814,     0.98137,     0.98134,     0.98131,     0.98129,     0.98126,     0.98123,\n",
      "             0.9812,     0.98117,     0.98114,     0.98109,     0.98102,     0.98095,     0.98088,     0.98081,        0.98,     0.97996,     0.97993,      0.9799,     0.97987,     0.97984,     0.97981,     0.97978,     0.97975,     0.97972,     0.97969,     0.97966,     0.97963,      0.9796,     0.98103,\n",
      "            0.98265,     0.98426,     0.98588,      0.9875,     0.98912,     0.99073,     0.99235,     0.99397,     0.99559,      0.9972,     0.99882,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
      "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.96739,     0.96739,     0.96739,     0.95652,     0.95652,     0.95652,     0.95652,     0.95652,     0.94565,     0.94565,     0.94565,     0.94565,     0.94565,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,\n",
      "            0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,\n",
      "            0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,\n",
      "            0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,\n",
      "            0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,\n",
      "            0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,\n",
      "            0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,\n",
      "            0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,\n",
      "            0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,\n",
      "            0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93478,     0.93315,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,\n",
      "            0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92391,     0.92363,     0.92315,     0.92267,     0.92219,     0.92171,     0.92124,     0.92076,     0.92028,      0.9198,\n",
      "            0.91932,     0.91884,     0.91836,     0.91788,      0.9174,     0.91692,     0.91645,     0.91597,     0.91549,     0.91501,     0.91453,     0.91405,     0.91357,     0.91309,     0.90684,     0.90217,     0.90217,     0.90217,     0.90217,     0.90217,     0.90217,     0.90217,     0.90217,\n",
      "            0.90217,     0.90217,     0.90217,     0.90217,      0.9021,     0.90169,     0.90128,     0.90087,     0.90046,     0.90006,     0.89965,     0.89924,     0.89883,     0.89842,     0.89801,      0.8976,     0.89719,     0.89678,     0.89637,     0.89596,     0.89556,     0.89515,     0.89474,\n",
      "            0.89433,     0.89392,     0.89351,      0.8931,     0.89269,     0.89228,     0.89187,     0.89147,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,      0.8913,     0.88486,     0.87983,     0.87818,     0.87653,     0.87489,\n",
      "            0.87324,     0.87159,     0.86994,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,\n",
      "            0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86957,     0.86871,     0.86641,      0.8641,      0.8618,      0.8595,      0.8587,      0.8587,\n",
      "             0.8587,      0.8587,      0.8587,      0.8587,      0.8587,      0.8587,      0.8587,      0.8587,      0.8587,     0.85553,     0.85205,     0.84856,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,\n",
      "            0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,\n",
      "            0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84783,     0.84613,     0.84124,\n",
      "            0.82583,     0.82379,     0.82175,     0.81971,     0.81766,     0.81562,     0.81522,     0.81332,     0.81004,     0.80675,     0.80397,     0.80258,     0.80118,     0.79979,     0.79839,       0.797,      0.7956,     0.79421,     0.78701,      0.7821,     0.78134,     0.78058,     0.77982,\n",
      "            0.77906,      0.7783,     0.77754,     0.77678,     0.77602,     0.77527,     0.77451,     0.77375,     0.77299,     0.77223,     0.77165,     0.77138,     0.77112,     0.77085,     0.77059,     0.77033,     0.77006,      0.7698,     0.76954,     0.76927,     0.76901,     0.76875,     0.76848,\n",
      "            0.76822,     0.76795,     0.76769,     0.76743,     0.76716,      0.7669,     0.76664,     0.76637,     0.76611,     0.76584,     0.76558,     0.76532,     0.76505,     0.76479,     0.76453,     0.76426,       0.764,     0.76373,     0.76347,     0.76321,     0.76294,     0.76268,     0.76242,\n",
      "            0.76215,     0.76189,     0.76163,     0.76136,      0.7611,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,     0.76087,\n",
      "            0.76087,     0.76087,     0.76047,     0.75984,     0.75921,     0.75858,     0.75795,     0.75733,      0.7567,     0.75607,     0.75544,     0.75481,     0.75419,     0.75356,     0.75293,      0.7523,     0.75167,     0.75105,     0.75042,     0.72698,     0.72316,     0.71934,     0.71663,\n",
      "            0.71509,     0.71354,     0.71199,     0.71045,      0.7089,     0.70736,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,     0.70652,\n",
      "            0.70652,     0.70652,     0.70623,     0.70579,     0.70535,     0.70491,     0.70447,     0.70403,     0.70358,     0.70314,      0.7027,     0.70226,     0.70182,     0.70138,     0.70094,      0.7005,     0.70006,     0.69962,     0.69917,     0.69873,     0.69829,     0.69785,     0.69741,\n",
      "            0.69697,     0.69653,     0.69609,     0.68475,     0.68263,      0.6805,     0.67837,     0.67625,     0.67412,     0.67282,     0.67162,     0.67041,      0.6692,       0.668,     0.66679,     0.66558,     0.66437,     0.66317,     0.66208,     0.66101,     0.65994,     0.65887,      0.6578,\n",
      "            0.65673,     0.65566,     0.65459,     0.65352,     0.65245,     0.65189,     0.65151,     0.65114,     0.65076,     0.65038,        0.65,     0.64962,     0.64925,     0.64887,     0.64849,     0.64811,     0.64773,     0.64735,     0.64698,      0.6466,     0.64622,     0.64584,     0.64546,\n",
      "            0.64509,     0.64471,     0.64433,     0.64395,     0.64357,      0.6432,     0.64282,     0.64244,     0.64206,     0.64168,      0.6413,      0.6413,      0.6413,      0.6413,      0.6413,      0.6413,      0.6413,      0.6413,      0.6413,      0.6413,      0.6413,      0.6413,      0.6413,\n",
      "             0.6413,      0.6413,      0.6413,      0.6413,     0.64074,     0.63932,      0.6379,     0.63649,     0.63507,     0.63366,     0.63224,     0.63082,     0.62877,     0.62647,     0.62417,     0.62187,     0.61957,     0.61843,      0.6173,     0.61616,     0.61503,     0.61389,     0.61276,\n",
      "            0.61162,     0.61049,     0.60935,      0.6082,     0.60701,     0.60583,     0.60465,     0.60346,     0.60228,      0.6011,     0.59991,     0.59873,     0.58568,     0.57711,     0.57531,     0.57442,     0.57353,     0.57265,     0.57176,     0.57087,     0.56999,      0.5691,     0.56821,\n",
      "            0.56733,     0.56644,     0.56555,     0.56391,     0.56179,     0.55967,     0.55756,     0.55544,     0.53248,     0.53166,     0.53084,     0.53002,     0.52921,     0.52839,     0.52757,     0.52675,     0.52593,     0.52511,     0.52429,     0.52347,     0.52265,     0.52183,     0.52174,\n",
      "            0.52174,     0.52174,     0.52174,     0.52174,     0.52174,     0.52174,     0.52174,     0.52174,     0.52174,     0.52174,     0.52174,     0.52148,     0.52054,     0.51959,     0.51865,     0.51771,     0.51676,     0.51582,     0.51487,     0.51393,     0.51299,     0.51204,      0.5111,\n",
      "            0.50926,     0.50715,     0.50504,     0.50292,     0.50081,     0.49168,     0.48733,     0.48511,     0.48288,     0.48066,     0.47844,     0.47043,     0.45985,     0.45572,     0.45461,      0.4535,     0.45238,     0.45127,     0.45016,     0.44904,     0.44793,     0.44682,      0.4457,\n",
      "            0.43314,      0.4262,     0.42026,     0.41481,     0.40921,     0.40354,     0.40048,     0.39826,     0.39603,      0.3938,     0.39157,     0.39011,     0.38875,      0.3874,     0.38604,     0.38469,     0.38333,     0.38197,     0.38062,     0.36295,     0.35799,     0.35648,     0.35496,\n",
      "            0.35344,     0.35192,      0.3504,     0.34889,     0.34697,     0.34414,     0.34131,     0.33848,     0.33422,      0.3283,     0.32201,      0.3155,     0.31108,     0.30676,     0.30307,     0.30018,     0.29729,      0.2944,      0.2814,     0.27964,     0.27787,     0.27611,     0.27434,\n",
      "            0.27258,     0.26745,     0.26054,     0.25884,     0.25713,     0.25543,     0.25372,     0.25202,     0.25031,     0.24824,     0.24609,     0.24393,     0.24178,     0.23962,      0.2334,     0.22756,     0.22531,     0.22305,     0.22079,     0.21853,     0.19502,     0.19076,     0.18651,\n",
      "            0.17689,     0.17312,     0.17209,     0.17106,     0.17003,       0.169,     0.16798,     0.16695,     0.16592,     0.16489,     0.16386,     0.16194,     0.15652,     0.14099,     0.13925,      0.1375,     0.13575,       0.134,     0.13226,     0.13051,     0.12934,      0.1282,     0.12706,\n",
      "            0.12593,     0.12479,     0.12365,     0.12251,     0.12137,     0.12023,     0.11198,    0.096011,    0.093796,    0.091581,    0.089365,     0.08715,    0.084815,    0.082469,    0.080122,    0.077776,    0.070429,    0.064686,    0.063969,    0.063253,    0.062537,     0.06182,    0.061104,\n",
      "           0.060387,    0.059671,    0.058954,    0.058238,    0.057521,    0.056805,    0.056088,    0.055372,    0.054655,    0.053245,    0.051314,    0.049383,    0.047451,     0.04552,    0.043589,    0.025392,    0.019634,    0.016085,    0.012535,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
      "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
      "fitness: np.float64(0.5827429752015134)\n",
      "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
      "maps: array([    0.54642])\n",
      "names: {0: 'licence'}\n",
      "nt_per_class: array([92])\n",
      "nt_per_image: array([86])\n",
      "results_dict: {'metrics/precision(B)': np.float64(0.8922667469694526), 'metrics/recall(B)': np.float64(0.8478260869565217), 'metrics/mAP50(B)': np.float64(0.9096818307640605), 'metrics/mAP50-95(B)': np.float64(0.5464164356945637), 'fitness': np.float64(0.5827429752015134)}\n",
      "save_dir: PosixPath('/Users/neslyn/Desktop/dl_project/runs/detect/train132')\n",
      "speed: {'preprocess': 0.70510417436856, 'inference': 447.82577277882683, 'loss': 0.0006802326615163406, 'postprocess': 0.5991710353301579}\n",
      "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
      "task: 'detect'\n"
     ]
    }
   ],
   "source": [
    "# train and validate model\n",
    "# from the table above I could see yolo v8 small performs better\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "model.train(data=\"dataset_split/data.yaml\", epochs=10, name=\"model_blur\")\n",
    "metrics = model.val(data=\"dataset_split/data.yaml\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bc72f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mmodel\u001b[49m.predict(source=\u001b[32m0\u001b[39m, conf=\u001b[32m0.4\u001b[39m, save=\u001b[38;5;28;01mTrue\u001b[39;00m, stream=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "results = model.predict(source=\"test.mp4\", conf=0.4, save=True, stream=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blurred video saved as output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"runs/detect/model_blur/weights/best.pt\")\n",
    "\n",
    "input_path = \"test.mp4\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(\"output.mp4\", fourcc, fps, (width, height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model.predict(source=frame, conf=0.4, stream=True, verbose=False)\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()\n",
    "\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "            \n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "            if roi.size > 0:\n",
    "                blur = cv2.GaussianBlur(roi, (35, 35), 0)\n",
    "                frame[y1:y2, x1:x2] = blur\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Blurred video saved as output.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
